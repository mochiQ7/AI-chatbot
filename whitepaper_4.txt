第4章　デジタルテクノロジーの課題と現状の対応策



第1節 AIの進化に伴う課題と現状の取組
　進化してきたAIは我々の生活に便利さをもたらす一方で、活用に当たっては留意すべきリスクや課題も存在している。これまで、AI全般についても、不適切なデータや偏ったデータを学習に使用することでモデルのバイアスや誤差が増加し、予測の信頼性が低下する点や、多くの従来の機械学習モデルについてブラックボックス（透明性の欠如）となっていてその内部動作が理解しにくく、重要な意思決定の場面で問題を引き起こす可能性が指摘されていた。これに加え、生成AIが爆発的に発展・普及する中で、特有の課題・リスクも明らかになってきた。以下に生成AIが抱えるリスク・課題を技術的/社会・経済的な観点から概観する。

1 生成AIが抱える課題
　2024年4月に総務省・経済産業省が策定した「AI事業者ガイドライン（第1.0版）」では、（従来から存在する）AIによるリスクに加えて、生成AIによって顕在化したリスクについて例示している。例えば、従来から存在するAIによるリスクとして、バイアスのある結果及び差別的な結果が出力されてしまう、フィルターバブル及びエコーチェンバー現象(フィルターバブル」とは、アルゴリズムがネット利用者個人の検索履歴やクリック履歴を分析し学習することで、個々のユーザーにとっては望むと望まざるとにかかわらず見たい情報が優先的に表示され、利用者の観点に合わない情報からは隔離され、自身の考え方や価値観の「バブル（泡）」の中に孤立するという情報環境を指す。「エコーチェンバー」とは、同じ意見を持つ人々が集まり、自分たちの意見を強化し合うことで、自分の意見を間違いないものと信じ込み、多様な視点に触れることができなくなってしまう現象を指す)が生じてしまう、データ汚染攻撃のリスク（AIの学習実施時の性能劣化及び誤分類につながるような学習データの混入等）、AIの利用拡大に伴う計算リソースの拡大によるエネルギー使用量及び環境負荷(同ガイドラインにおいては、エネルギー管理にAIを導入することで、効率的な電力利用も可能となる等、AIによる環境への貢献可能性もある点も指摘されている)等が挙げられている。また、生成AIによって顕在化したリスクとしては、ハルシネーション等が挙げられる。生成AIは事実に基づかない誤った情報をもっともらしく生成することがあり、これをハルシネーション（幻覚）と呼ぶ。技術的な対策が検討されているものの完全に抑制できるものではないため、生成AIを活用する際には、ハルシネーションが起こる可能性を念頭に置き、検索を併用するなど、ユーザーは生成AIの出力した答えが正しいかどうかを確認することが望ましい。また、生成AIの利用において、個人情報や機密情報がプロンプトとして入力され、そのAIからの出力等を通じて流出してしまうリスクや、ディープフェイクによる偽画像及び偽動画といった偽・誤情報を鵜呑みにしてしまい、情報操作や世論工作に使われるといったリスク、既存の情報に基づいてAIにより生成された回答を鵜呑みにする状況が続くと、既存の情報に含まれる偏見を増幅し、不公平あるいは差別的な出力が継続/拡大する（バイアスを再生成する）リスクがあること等も指摘されている。
　同ガイドラインでは、このような「リスクの存在を理由として直ちにAIの開発・提供・利用を妨げるものではない」としたうえで、「リスクを認識し、リスクの許容性及び便益とのバランスを検討したうえで、積極的にAIの開発・提供・利用を行うことを通じて、競争力の強化、価値の創出、ひいてはイノベーションに繋げることが期待される」としている。

 1 主要なLLMの概要
　生成AIの基盤となる大規模言語モデル（LLM）の開発では、マイクロソフトやグーグルなど米国ビックテック企業などが先行している状況にある。
　しかし、日本以外の企業・研究機関がクローズに研究開発を進めたLLMを活用するだけでは、LLM構築の過程がブラックボックス化してしまい、LLMを活用する際の権利侵害や情報漏えいなどの懸念を払拭できない。日本語に強いLLMの利活用のためには、構築の過程や用いるデータが明らかな、透明性の高い安心して利活用できる国産のLLM構築が必要となる＊3。すでに日本の企業においても、独自にLLM開発に取り組んでおり、ここではその動向を紹介する。ビッグテック企業が開発したLLMと比べると、日本では、中規模モデルのLLMが開発されている傾向が見られる。

2 国産LLMの開発

ア　NICTによる国産LLMの開発
　2023年7月に、国立研究開発法人情報通信研究機構（NICT）は、ノイズに相当するテキストが少ない350GBの高品質な独自の日本語Webテキストを用いて、400億パラメータの生成系の大規模言語モデルを開発した旨を発表した。発表によれば、NICTの開発したLLMについてはファインチューニングや強化学習は未実施であり、性能面ではChatGPT等と比較できるレベルではないものの、日本語でのやり取りが可能な水準に到達しているとしており、今後は、学習テキストについて、日本語を中心として更に大規模化していくこととしている。また、GPT-3と同規模の1,790億パラメータのモデルの事前学習に取り組み、適切な学習の設定等を探索していく予定である。さらに、より大規模な事前学習用データ、大規模な言語モデルの構築に際し、ポジティブ・ネガティブ両方の要素に関して改善を図るとともに、WISDOM X、MICSUS等既存のアプリケーションやシステムの高度化等にも取り組む予定としている（2024年5月現在、NICTではさらに開発を進め、最大3,110億パラメータのLLMを開発するなど、複数種類のLLMを開発しパラメータや学習データの違いによる性能への影響等を研究している）。

イ　サイバーエージェントが開発した日本語LLM「CyberAgentLM」
　2023年5月、サイバーエージェントが最大68億パラメータの日本語LLMを開発したことを発表した。2023年11月には、より高性能な70億パラメータ、32,000トークン対応の日本語LLM「CyberAgentLM2-7B」と、チャット形式でチューニングを行った「CyberAgentLM2-7B-Chat」の種類を公開した。日本語の文章として約50,000文字相当の大容量テキストを処理可能である。商用利用が可能なApacheLicense2.0で提供されている。

ウ　日本電信電話（NTT）が開発した日本語LLM「tsuzumi」
　2023年11月にNTTが開発した、軽量かつ世界トップレベルの日本語処理能力を持つLLMモデル「tsuzumi」が発表された。「tsuzumi」のパラメータサイズは6～70億と軽量であり、クラウド提供型LLMの課題である学習やチューニングに必要なコストを低減できる。「tsuzumi」は英語と日本語に対応しているほか、視覚や聴覚などのモーダルに対応し、特定の業界や企業組織に特化したチューニングが可能である。2024年3月から商用サービスが開始されており、今後はチューニング機能の充実やマルチモーダルの実装も順次展開される見込みである。


2 生成AIが及ぼす課題

　前述のような生成AI自身が抱える制約事項のほか、生成AIの進展・普及には、それに伴う社会的・経済的な課題も多く、国内外のテック事業者、プラットフォーム事業者、業界団体や政府等による対策検討が進められている。

1 偽・誤情報の流通・拡散等の課題及び対策
　「ディープフェイク」とは、「ディープラーニング（深層学習）」と「フェイク（偽物）」を組み合わせた造語で、本物又は真実であるかのように誤って表示し、人々が発言又は行動していない言動を行っているかのような描写をすることを特徴とする、AI技術を用いて合成された音声、画像あるいは動画コンテンツのことをいう。近年、世界各国でこれらディープフェイクによる情報操作や犯罪利用が増加しており、その対策には各方面からの取組が行われているものの、いたちごっこの様相を呈している。

ア　ディープフェイクによる課題
（ア）AIにより生成された偽・誤情報の流通・拡散
　生成AIの進歩により、非常に高品質なテキスト、画像、音声、動画を生成することが可能になり、リアルで信憑性の高い偽・誤情報を作成することが可能になった。ディープフェイク技術を用いれば、実在する人物が実際には言っていないことを本当に話しているかのような動画を簡単に作成することができる。我が国でも、生成AIを利用して作られた岸田総理大臣の偽動画がSNS上で拡散した事例が発生した。2024年1月1日に発生した能登半島地震の際にも、東日本大震災の時の津波映像や静岡県熱海市で2021年に起きた大規模土石流の映像などをあたかも能登半島地震と結びつけた投稿がSNS上で多数投稿され、大量に閲覧・拡散された。2020年には、新型コロナウイルス感染症と5G電波との関係を謳う偽情報が携帯電話基地局の破壊活動を招くなど社会的影響も生じさせている。
　SNSなど様々なデジタルサービスが普及し、あらゆる主体が情報の発信者となり、インターネット上では膨大な情報やデータが流通するようになったが、このような情報過多の社会においては、供給される情報量に比して、我々が支払えるアテンションないし消費時間が希少となるため、それらが経済的価値を持って市場で流通するようになる。このことはアテンション・エコノミーと呼ばれ、プラットフォーム事業者が、受信者のアテンションを得やすい刺激的な情報を優先表示するようになるなど、経済的インセンティブ（広告収入）により偽・誤情報が発信・拡散されたり、インターネット上での炎上を助長させたりする構造となっている。
　偽・誤情報の拡散は世界的に問題となっており、2024年1月、世界経済フォーラムは、社会や政治の分断を拡大させるおそれがあるとして、今後2年間で予想される最も深刻なリスクとして「偽情報」を挙げた。特に2024年は、米国をはじめ、バングラデシュ、インドネシア、パキスタン、インド等、50か国余りで国政選挙が予定されている。既にインドネシア大統領選の際のディープフェイク動画の流布や、米大統領選の予備選の前に偽の音声でバイデン米大統領になりすます悪質な電話等、生成AIを利用したディープフェイクによる情報操作の事例が確認されている。

（イ）その他犯罪利用
　生成AIが、情報操作のみならず、犯罪に利用されるケースも増えている。米国OpenAIのチャットボット（自動会話プログラム）であるChatGPTに用いられているものと同じAIが悪用され、「悪いGPT（BadGPT）」や「詐欺GPT（FraudGPT）」と呼ばれる不正チャットボットによってフィッシング詐欺メールが量産されている。このようなハッキングツールは、OpenAIがChatGPTを公開した2022年11月の数か月後には闇サイト上で確認されるようになり、ChatGPT公開後の12か月間で、フィッシング詐欺メールは1,265％増加し、一日平均約3万1,000件のフィッシング攻撃が発生しているという試算もある。
　ディープフェイクを利用した犯罪には、AIの画像生成能力を悪用した恐喝行為もある。SNS等で共有された一般的な写真画像をAIで不適切な内容に変換し、被害者を脅迫するというもので、米国連邦捜査局（FBI）は、被害者には未成年の子供も含まれると警告している。

イ　ディープフェイクによる情報操作や犯罪利用への対策

（ア）欧州連合（EU）
　偽・誤情報に関する法規制で先行するのは欧州連合（以下「EU」という。）である。2022年11月に発効した「デジタルサービス法（The Digital Services Act）」（以下「DSA」という。）は、超大規模オンラインプラットフォーム（VLOP）などに対して、自身の提供するサービスのリスク評価（偽情報に関するものを含む）やリスク軽減措置の実施を義務付けており、違反企業には最大で世界年間売上高の6％の制裁金が科されることとなっている。実際に、EUの執行機関である欧州委員会（以下「EC」という。）は、イスラエルに対するハマス等によるテロ攻撃に関わる違法コンテンツの拡散等を踏まえ、X（旧Twitter）がDSAを遵守していない可能性があるとして、違法コンテンツの拡散への対応のほか、プラットフォーム上の情報操作への対抗措置の有効性等の領域について、2023年12月に正式な調査を開始した。プラットフォーム上の情報操作への対抗措置に関し、ECは、特に、投稿に第三者が匿名で注釈を加える「コミュニティ・ノート」という機能等の有効性に焦点を当てる方針であるとしている。2024年3月、欧州議会は、AIに関する世界初の包括的な法的枠組みと位置づける「AI法（AI Act）」の最終案を可決し、同年5月にEU理事会にて正式承認され、同法が成立した。同法は一部ディープフェイクに関する規制も含み、2026年頃には本格的に適用される見込みである。

（イ）英国
　英国では、2023年10月に発効された「オンライン安全法（Online Safety Act 2023）」に、虚偽であると知っている情報を受信者に心理的または身体的危害を与えることを意図してインターネット上で送信した者に、6か月の禁錮刑を科す内容が含まれている。特に、相手に苦痛、不安や屈辱等を与える加害意図や、自分が性的満足を得ようとする意図があったと立証されれば、最高刑が懲役2年となる。

（ウ）米国
　米国においては、2023年7月、バイデン政権が、AI開発を主導するGoogle、Meta PlatformsやOpenAI等の7社から、AIの安全性や透明性向上に取り組む自主的なコミットメントを得たと発表した。同年9月には、新たにIBM、Adobe、NVIDIA等8社が合意し、同15社はディープフェイク対策として、真贋を示す目印をデータに忍ばせて識別を可能にする「電子透かし」等、AIによる生成を識別するための技術開発を推進している。また、米国の一部の州において、ポルノや選挙活動等の特定の目的下でのディープフェイクに関する規制が見られる。例えば、カリフォルニア、テキサス、イリノイ、ニューヨーク等9州では、相手の同意の無いディープフェイクを用いたポルノ画像や動画の配布を刑事犯罪として規定しているほか、テキサス州やカリフォルニア州では、公職の候補者に対するディープフェイク等の発信に係る規制法を設けている。なお、米国連邦法においては、国防総省や全米科学財団等の連邦機関に対し、ディープフェイクを含む偽情報に関する調査研究の強化等を求める法律が制定されている。他方、民間事業者に対しては、1996年成立の「通信品位法（Communications Decency Act）」第230条（通称Section 230）において、プロバイダは第三者が発信する情報に原則として責任を負わず、有害な内容の削除に責任を問われないと規定されているが、バイデン政権では、偽・誤情報に関してプラットフォーム事業者に一定の責任を求めるよう、法改正しようとする方向で議論が行われている。

（エ）日本
　我が国におけるデジタル空間の情報流通の健全性確保に向けては、総務省が2023年11月から「デジタル空間における情報流通の健全性確保の在り方に関する検討会」を開催しており、2024年（令和6年）夏頃までに一定のとりまとめを公表予定である。
　技術的な対策としては、インターネット上のニュース記事や広告などの情報コンテンツに、発信者情報を紐付けるオリジネータープロファイル（OP、Originator Profile）技術の研究開発が進んでいる。この技術により、なりすましや改変が見える化されることで、Web利用者が透明性の高いコンテンツを閲覧できるようになる、フェイクニュースや安易な関心獲得による広告収益が得られにくくなり、適正なWebメディアやコンテンツの配信者の権利利益侵害を低減できるようになる、広告枠が設置されるWebコンテンツの発信者が明確になることで、広告主が安心して広告出稿ができるようになるといった効果が期待される。
　また、国立情報学研究所（以下「NII」という。）がフェイク技術対策に関する研究に早期から取り組んでおり、2021年9月には、AIにより生成されたフェイク顔画像を自動判定するツール「SYNTHETIQ VISION：Synthetic video detector」を開発した。これは真贋判定をしたい画像をサーバーにアップロードすると、同ツールがフェイクかどうかを判定するものである。現在NIIでは、更に進んだディープフェイク対策技術「Cyber Vaccine（サイバーワクチン）」を開発中であり、これが実現すると、真贋判定だけでなく、どこが改竄されたのか等の情報も得ることができるようになると期待されている。


2 著作権を含む知的財産権等に関する議論
　生成AIの生成物は、主に、文章、画像、音楽・音声の3種類である。これらは、大量のデータからその特徴を学習し、プロンプト（入力）に応じて適切な結果を出力する「機械学習」の手法を用いて開発されている。この際、データを収集・複製し、学習用データセットを作成したり、データセットを学習に利用して、AI（学習済みモデル）を開発することがオリジナルデータの制作者等の権利を侵害しないかという開発・学習段階の論点がある。また、生成AIを利用して画像等を生成したり、生成した画像等をアップロードして公表、生成した画像等の複製物（イラスト集など）を販売する際に、既存の画像等の作品と類似したものを使ってしまう等の場合に、既存作品の制作者の権利の侵害等になることがある（生成・利用段階の論点）。

ア　生成AIの進展・普及に伴う著作権を含む知的財産権等に関わる問題提起
　生成AIに関連する著作権や肖像権の侵害問題は国際的に注目されており、多くの訴訟が発生している。米国では、2022年11月、GitHub Copilotの開発に関連して、学習に使用しているオープンソースコードがプログラマーの著作権を侵害している可能性があるとして、Microsoft、GitHub、OpenAIに対する集団訴訟が提訴されたほか、2023年7月には、米国の作家3名がOpenAIとMeta Platformsの2社を提訴した訴訟も発生した。同集団訴訟は、ChatGPTの機械学習に作家の著作物が無断で使用されたことによる損害賠償を請求するもので、同訴訟の結果、OpenAIは学習データから著作物を削除するのではなく、著作権侵害で訴えられた場合の訴訟費用を負担することを表明することとなった。
　新聞社、通信社等のメディアでのAIの活用は慎重なものとなっている。米国のAssociated Press（AP通信）は2023年7月にOpenAIとの提携を発表し、生成AIをニュース報道に生かす方法等について共同で研究する契約を結んだが、8月にはAIを配信可能なコンテンツ作成のために使用しないとした。一方、New York TimesはAIによる記事の無断使用でOpenAIとMicrosoftを訴え、これが報道機関による初の訴訟提起となった。日本国内においても、新聞・通信各社は、生成AIによる報道記事の無断使用について、生成AIによる記事の無断使用は許容できず、根本的な法改正に向けた検討を求める意見を表明している。
　日本では、生成AI技術の発展と急速な普及に伴って権利者やAI開発者から著作権などの知的財産権の侵害に関する懸念の声が上がったことを踏まえ、2024年3月、文化審議会著作権分科会法制度小委員会において、「AIと著作権に関する考え方について」がとりまとめられるとともに、（著作権を含む）知的財産権との関係について、2024年5月、AI時代の知的財産権検討会より、「AI時代の知的財産権検討会　中間とりまとめ」が公表された。

イ　著作権を含む知的財産権等の侵害リスクに対する取組
　生成AIの利用に際しての著作権等の権利侵害対策に向けては、データ・コンテンツの権利保持者とAI事業者双方が、互いの契約の中で対応を行うこと等が考えられる。技術的には、生成AI生成物であることの表示を可能とする電子透かしの実用化や、OpenAIによる知的財産権を侵害する恐れのあるデータ・コンテンツのAI入出力を抑制する仕様の提供等がある一方で、New York Times、CNN、Bloomberg、Reuters、日本経済新聞等の国内外のメディア側も、OpenAI等AI事業者のGPTボットのブロックを行う等の対策で自衛している。
　技術を活用しながら著作権侵害の法的リスクに対してコミットする取組もある。Microsoftは、大規模言語モデル（Large Language Model：LLM）を組み込んだ自社の生産性向上ツール「Microsoft Copilot」に対する法的リスクに対して責任を負う、「Copilot Copyright Commitment」を2023年9月に発表している。Microsoft Copilotで生成した出力結果を使用して、著作権上の異議を申し立てられた場合、Microsoftが責任をとる仕組みとなっている。著作物を使用しない、あるいは許諾済みの著作物を活用する方法で著作権等侵害のリスクを回避する方法もある。例えば、Adobeが提供する「Adobe Firefly」は、オープンライセンス等、著作権の問題の無い画像を学習段階で利用しており、著作権侵害の心配なく生成した画像の商用利用が可能としている。




第2節 AIに関する各国の対応

　こうした生成AIをはじめとするAIの急速な普及のなかで生じた倫理的・社会的な課題に対処するためには、国内のみならず、諸外国と協調した取組が必要である。

1 国際的な議論の動向

1 広島AIプロセス
　AIについての倫理的・社会的課題に対する議論は2015年頃から活発化しており、我が国は、早期からG7/G20や経済協力開発機構（以下「OECD」という。）等における議論を先導し、AI原則の策定に重要な役割を果たしてきた。2016年4月に高松で開催されたG7情報通信大臣会合において、日本からAIの開発原則に関する議論が提案され、その後OECDで合意されたAI原則が2019年5月に公開されたことを受けて、同年6月のG20首脳会合にて、「G20 AI原則」が合意された。2019～2020年には、AI原則については国際的なコンセンサスが形成されつつあり、同原則を社会に実装するための具体的な制度や規律の策定に関する議論に移行している。更には、2022年の生成AIの急速な普及により、G7等の国際協調の場においても、また各国においても、AIガバナンスの議論が活発化している。
　2023年4月、群馬県高崎市でG7群馬高崎デジタル・技術大臣会合が開催され、生成AIの急速な普及と進展を背景に、「責任あるAIとAIガバナンスの推進」などについて議論が交わされた。同会合では、G7のメンバー間で異なる、AIガバナンスの枠組み間の相互運用性の重要性が確認され、「責任あるAIとAIガバナンスの推進」、「安全で強靱性のあるデジタルインフラ」、「自由でオープンなインターネットの維持・推進」等の6つのテーマからなる閣僚宣言が取りまとめられた。同宣言はその後、5月に広島で開催された主要7か国首脳会議（G7広島サミット）における議論に反映され、当該サミットの首脳コミュニケ（宣言）において、生成AIに関する議論のための広島AIプロセスの創設が指示された。具体的には、OECDやGPAI（後述）等の関係機関と協力し、G7の作業部会にて調査・検討を進めることとなった。
　2023年9月には、7月～8月にOECDが起草したレポートや、生成AI等を含む高度なAIシステムの開発に関して議論すべく閣僚級会合が開催され、透明性、偽情報、知的財産権、プライバシーと個人情報保護等が優先課題であることが確認された。その後10月30日に「広島AIプロセスに関するG7首脳声明」が発出され、まずは高度なAIシステムの開発者を対象とした国際指針と行動規範が公表された。更に同年12月には、AIに関するプロジェクトベースの協力を含む広島AIプロセス包括的政策枠組みや、広島AIプロセスを前進させるための作業計画が発表されている。

2 OECD／GPAI／UNESCOの動き

ア　OECD
　OECD、GPAI、UNESCO等、多くの国際機関もグローバルな観点からAIガバナンス制度の検討を進めている。2019年5月にOECDのAI原則が公開されて以降、各種OECDレポートの発表やプロジェクトの推進等、G7との連携の下、積極的な活動が行われている。また、OECD、GPAI、UNESCOの3機関は、2023年9月に「生成AI時代の信頼に関するグローバルチャレンジ（Global Challenge to Build Trust in the Age of Generative AI）」を発表し、G7の包括枠組みを踏まえ、偽情報やディープフェイク等による社会的リスクに対し、イノベーティブな解決策を進めるグローバルな連携プロジェクトを推進している。
　2024年5月に開催されたOECD閣僚理事会では、生成AIに関するサイドイベント「安全、安心で信頼できるAIに向けて：包摂的なグローバルAIガバナンスの促進」において、岸田総理大臣から49か国・地域の参加を得て広島AIプロセスの精神に賛同する国々の自発的な枠組みである「広島AIプロセス　フレンズグループ」を立ち上げることを発表した。

イ　GPAI
　「AIに関するグローバルパートナーシップ（Global Partnership on AI）」（以下「GPAI」という。）は、2020年、人間中心の考え方に立ち、「責任あるAI」の開発・利用を実現するために、OECDとG7の共同声明により創設された。同組織は、OECDが事務局を務め、価値観を共有する政府、国際機関、産業界、有識者等からなる官民国際連携組織で、現在29か国が参加している。GPAIには、「責任あるAI」、「データ・ガバナンス」、「仕事の未来」、「イノベーションと商業化」という4つの研究部会が設置されており、専門家による議論と実践的な調査が実施されている。GPAIの年次サミットである「GPAIサミット2023」においては、新たなGPAI専門家支援センターである、GPAI東京専門家支援センターの立ち上げが承認された。同センターでは、生成AIに関する調査・分析等のプロジェクトを先行的に実施する予定となっている。

ウ　UNESCO
　国連教育科学文化機関（UNESCO）も、2021年にAIの倫理に関する勧告「UNESCO Recommendation on the Ethics of Artificial Intelligence」を採択し、各国における取組を支援している。2023年9月には、教育・研究に関する初の生成AIのグローバルガイダンスである「教育・研究分野における生成AIのガイダンス（Guidance for generative AI in education and research）」を公表し、生成AIの定義や説明、倫理的及び政策的な論点と教育分野への示唆、規制の検討に必要なステップ、カリキュラムデザインや学習等について紹介している。ほとんどの生成AIが主として大人向けに設計されていることから、教育現場での使用は13歳以上に制限すべきと提案し、各国政府には、データのプライバシー保護を含む適切な規制や教員研修等を求めている。


3 AI安全性サミット
2023年5月、OpenAIは、今後10年以内に人間の専門家のスキルレベルを超えるAIシステムが実現する可能性があると発表した。同社はこれを「フロンティアAI（Frontier AI）」と命名し、核エネルギーや合成生物学等の人類の存在上のリスクに鑑みて、事後的対応ではなく国際的な規制を検討すべきとした。これを受けてスナク英国首相は、2023年11月1日～2日に英国ブレッチリーにて、「AI安全性サミット」を開催した。従来の人権や公平性といった「AI倫理」を超えて、AIによる「深刻且つ破滅的な危害」の防止を視野に入れた「AIの安全性」について議論されたことが特徴的である。
　本サミットの成果文書として「ブレッチリー宣言」が採択された。また、英国はAIセーフティ・インスティテュートを設置することについても決定した。
　2024年5月21日～22日には、韓国・英国共催により「AIソウル・サミット」が開催された（21日の首脳セッションはオンライン開催、22日の閣僚セッションはソウルで対面開催）。AI安全性の議論を深めるとともに、AI開発におけるイノベーション促進及びAIの恩恵の公平な享受について議論が行われ、首脳級の成果文書として「安全、革新的で包摂的なAIのためのソウル宣言」及び付録「AI安全性の科学に関する国際協力に向けたソウル意図表明」、閣僚級の成果文書として「安全、革新的で包摂的なAIの発展のためのソウル閣僚声明」が採択された。今後、2025年2月にフランスにて次回会合が開催される予定となっている。


4 国際連合の動向
　前項のとおり、フロンティアAIに対する国際的なガバナンス体制への関心の高まりを受けて、2023年7月の国連安全保障理事会においては、英国主導でAIに関する議論が行われた。グテーレス国連事務総長は同年10月に、事務総長の諮問機関として、AIハイレベル諮問機関を立ち上げ、日本人の構成員も参加している。また、2024年3月21日、国連総会において、日本も共同提案国である、「持続可能な開発のための安全、安心で信頼できるAIシステムに係る機会確保に関する決議」＊9をコンセンサスで採択し、同決議案は、安全、安心で信頼できるAIに関する初めての国連総会決議となった。同決議案は「持続可能な開発のための2030アジェンダ」の達成に向けた進捗を加速し、デジタルディバイドを解消するため、安全、安心で信頼できるAIを促進しており、加盟国に対し、安全、安心で信頼できるAIに関連する規制・ガバナンスアプローチの策定・支持を推奨している。さらに、加盟国及びステークホルダーに対し、AI設計・開発中のリスク特定・評価・軽減のためのイノベーション促進や、データ保全のためのリスク管理メカニズムの策定・実施・公表等の手段を通じて、AIシステムが世界の課題に対応できるための環境を整備するよう推奨している。また、AIシステムのライフサイクルを通じて、人権及び基本的自由が尊重され、保護され、促進されるべきことを強調している。
　同決議案は、AIの国際ルールづくりに向け、広島AIプロセスをはじめ、G7やG20、OECD等で進めてきた議論を反映したものであり、国連総会決議には国際法上の拘束力はないものの、コンセンサスで加盟国が採択したということから、国際社会の総意としての政治的な重みを持つものである。


2 各国における法規制・ガイドライン等の整備動向

　現在、AIに関する法制度や国際標準に関する議論が世界各国で活発に行われており、2023年はEUのAI法の欧州議会での採択、米国のAIの安全性に係る大統領令、日本のAI関連事業者向けのガイドライン案の公表など、AI政策にとっては大きな節目となる年となった。それぞれの国、地域におけるAIに関する規制の動きを見ると、生成AIに対する急速な関心の高まりを受けて、各国・地域ではそれまで検討してきたガバナンス制度の見直しが求められている。進化が速い技術に関する規制の整備においては、各国政府が主導しつつも、AI事業者側の自主的な取組も必要であり、官民両輪で進められているところである。

1 欧州連合（EU）
　域内発のビッグテック企業が無い欧州は、他の地域に先駆けて最も厳しい規制を志向し、2020年からAIの規制に関する議論を続けてきた。2024年5月21日には、欧州市場でAIシステムを開発・提供・利用する事業者を対象とする、法的拘束力を持つ世界初の包括的なAI規制法と位置付けられるAI法（AI Act）（以下「AI法」という。）が成立した。AIの規制に関する包括的な法律の成立は主要国・地域で初めてとされており、今後段階的に適用が開始され、2026年頃には本格的に適用される見込みである。
　AI法は、リスクに応じて規制内容を変える「リスクベースアプローチ」という方針に基づいている。規制対象を、①許容できないリスク、②高いリスク、③限定的なリスク、④最小限のリスク、という4段階のリスクレベルのAIアプリケーション及びシステムに分類し、それぞれに対して異なる規制を課すこととしており、上記の規制に違反した事業者には、最も重い違反の場合、最高で3,500万ユーロ（約56億円）の罰金、あるいは年間売上高の7％の制裁金が科される可能性がある。

2 米国
　ビッグテック企業を多く保有する米国は、自国の企業保護に力を入れ、政府による規制よりも民間での自主的な対応を優先し、企業の取組に任せつつ必要の場合に政府が規制をかけるという立場をとってきた。民間側の取組として2023年7月、AI開発で先行する7社（Google、Meta PlatformsやOpenAI等）がAIの安全な開発のための自主的な取組を約束したこと、更に9月には新たな8社（IBM、Adobe、NVIDIA等）がそれに合意したことを米国政府が発表した。各社は、自主的なコミットメント（Voluntary Commitments）として、安全性、セキュリティ、信頼性の3つの観点から原則を掲げている。
　ホワイトハウスは、強制力のある規制が導入されるまで、各社が上記の取組を続けるとしていたが、その3か月後となる2023年10月30日、バイデン大統領は、「安全・安心・信頼できるAIの開発と利用に関する大統領令（Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence）」を発表した。対象とするAIの問題については、従来の倫理的観点から、安全保障問題に範囲を拡充しており、対象となる事業者はビッグテック企業に限らず、バイオテクノロジー企業等、国家の安全保障や経済に影響を及ぼす可能性のあるサービスや製品を取り扱う企業も含まれる。その内容については、AIに関する新たな安全性評価、公平性と公民権に関するガイダンス、またAIが労働市場に与える影響に関する調査等を義務付けるものであり、AIの安全性とセキュリティのための新しい基準、米国民のプライバシー保護、公平性と公民権の推進等をその主要な構成要素としている。
　大統領令の発表に引き続き、同年11月には、ハリス副大統領が、先述の英国AI安全性サミットにて「安全で責任あるAI利用の新イニシアチブ（New U.S. Initiatives to Advance the Safe and Responsible Use of Artificial Intelligence）」を発表し、その中で、大統領令の内容を具体化するべく、「米国AI安全研究所（AI Safety Institute）」（以下「US AISI」という。）を設置するとした。US AISIは、国立標準技術研究所（National Institute of Standards and Technology：NIST）内に設置され、危険な機能を評価及び軽減するためのガイドライン、ツール、ベンチマーク、ベスト プラクティスを作成し、AIリスクを特定して軽減するためのレッドチームを含む評価を実施する。また、人間が作成したコンテンツの認証、AIが生成したコンテンツの電子透かし、有害なアルゴリズムによる差別の特定と軽減や透明性の確保、プライバシー保護の導入等に係る技術的なガイダンスを開発する予定である。英国のAI安全研究所を含む国際的な同業機関との情報共有や研究協力、更には市民社会、学界、産業界の外部専門家との提携も可能となる。
　一方、連邦議会でも連邦レベルでのAI規制に関する法案が議論されている。2023年6月には、上院が、AIの急速な進歩に連邦議会が対応するための包括的な枠組みである「安全なイノベーション枠組み（SAFE Innovation Framework）」を提唱し、同年12月までに産業界の代表や有識者を招いたテーマ別のフォーラムを9回にわたって開催した。他方の下院は、2024年2月、AIに関する超党派のタスクフォースを設立すると発表し、AI政策の指針となる原則や政策提言を含む包括的な報告書を作成する予定となっている。上下両院では、選挙等の個別分野でのAI利用を規制する法案が複数提出されているものの、未だ議会を通過したものは無い。2024年秋に大統領選挙を控える米国では、生成AIの普及に伴うディープフェイクによる情報操作等の課題に直面し、AIの規制に関する議論が益々活発化するものと予想される。

3 英国
　英国は米国と中国に次いでAI研究が盛んな国とされており、AI分野への民間投資額においても、シンガポールの躍進により2023年に初めて4位に転落したものの、2019年以来、米国・中国に次いで世界3位を保ってきた。現スナク政権は、法的拘束力のあるAI規制には消極的で、安全に配慮しながらAIシステムの開発を促し、経済成長に繋げたいとする考えから、当面はEUのAI法のような厳格な規制を新たに整備せず、既存の枠組みで柔軟に対処する方針を表明してきた。同方針を踏まえ、英国政府が2023年3月に公表した政策文書「プロイノベーティブな規制手法（A pro-innovation approach to AI regulation）」が、同国のAI規制の基本的な枠組みに位置付けられている。同文書では、セキュリティ、透明性、公平性、説明責任、争議可能性の観点から5つの原則が掲げられており、AIガバナンスに取り組むに当たっては、「イノベーション促進型の、柔軟で法規制に縛られない、比例的で信頼できる、順応性があり、明確で且つ協力的な（proinnovation, flexible, non-statutory, proportionate, trustworthy, adaptable, clear and collaborative）」アプローチをとるとしている。当面は既存の法規制の下、各政府機関の連携により、産業界に対して上記原則の実装を促しつつ、将来的には、原則について何らかの義務化を図る可能性があるとしている。
　また、2023年11月27日、英国国家サイバーセキュリティセンター（National Cyber Security Centre：NCSC）と米国サイバーセキュリティ・インフラストラクチャー安全保障庁（Cybersecurity and Infrastructure Security Agency：CISA）が中心となり、日本を含む18か国が共同で、AIシステムのセキュリティガイドラインである「セキュアAIシステム開発ガイドライン（Guidelines for secure AI system development）」＊28を公表した。同ガイドラインでは、AIの設計、開発、導入、運用とメンテナンスの各段階において、取り組むべき事項を取りまとめている。

4 日本
　日本は、民主主義や基本的人権等の観点からは欧米と同様の立場である一方、文化や社会規範の差異により、AIに対する社会認識という点では、欧米とは異なる文化圏にある。これにより、AIガバナンスの方向性として、欧州が法的拘束力の強いハードローを志向しているのに対し、日本は現時点では、AIガバナンスに関する横断的な法規制によるアプローチではなく、民間事業者の自主的な取組を重んじるソフトローアプローチを志向しており、総務省と経済産業省を中心に取組が行われてきたところである。総務省のAIネットワーク社会推進会議による「AI開発ガイドライン」が2017年に、「AI利活用ガイドライン」が2019年に公表され、また同年3月に内閣府の統合イノベーション戦略推進会議が決定した、「人間中心のAI社会原則」を基にしたガイドラインが策定された。続いて2021年7月に経済産業省が公表した「AI原則実践のためのガバナンス・ガイドライン」（2022年1月に改訂）では、AI事業者が実施すべき行動目標が実践例と共に示されている。同ガイドラインは、AIを開発・運用する事業者が参考にし得るよう、環境・リスク分析やシステムデザイン、運用等の項目毎にまとめられている。
　2023年5月、政府は「AI戦略会議」を設置し、AIのリスクへの対応、AIの最適な利用に向けた取組、AIの開発力強化に向けた方策等、様々なテーマで議論を行い、「AIに関する暫定的な論点整理」を公表すると共に、各省庁のガイドラインの統合に向けた作業を進めることとされた。同年9月には、同会議にて生成AIに対するガバナンスも含めて統合された「新AI事業者ガイドライン スケルトン（案）」が示され、そして12月、政府は「AI事業者ガイドライン案」を公表した。同案では、人権への配慮や偽情報対策を求め、安全性やプライバシー保護等の10原則を掲げ、人間の意思決定や認知・感情を不当に操作するものは開発させないとしているが、欧米のような一定の法的拘束力を持つものではない。同案はその後、一般からの意見の公募を経て、2024年4月19日に「AI事業者ガイドライン（第1.0版）」として公表された。
　また、2023年12月のAI戦略会議において、岸田総理大臣は、AIの安全性に対する国際的な関心の高まりを踏まえ、AIの安全性の評価手法の検討等を行う機関として、米国や英国と同様に、日本にも「AIセーフティ・インスティテュート（AI Safety Institute）」（以下「AISI」という。）を設立すると発表し、2024年2月14日、経済産業省所管の情報処理推進機構（Informationtechnology Promotion Agency：IPA）に設置された。AISIは、英国・米国等の同様の機関とも連携しつつ、AIの開発・提供・利用の安全性向上に資する基準・ガイダンス等の検討、AIの安全性評価方法等の調査、AIの安全性に関する技術・事例の調査などを行っていくこととしている。




第3節　その他デジタルテクノロジーに関する議論の動向

1 メタバース、ロボティクス、自動運転に関する議論の動向

1 メタバース
　総務省の「Web3時代に向けたメタバース等の利活用に関する研究会」が2023年7月に取りまとめた報告書においては、メタバースに関する課題は、「メタバース空間内に係る課題」と「メタバース空間外と関連する課題」の2つに大別されている。
　メタバース空間内に係る課題については、①アバターに係る課題、②プラットフォーム間の相互運用性、③メタバース構築時・利活用時に係る課題、④データの取得・利用に係る課題が、メタバース空間外と関連する課題については、⑤ユーザーインターフェース（UI）/ユーザー体験（UX）に係る課題、⑥メタバースの動向/社会的な影響が挙げられた。同研究会ではこれらの課題について検討し、①～④の課題に対する取組の方向性としてメタバースの理念に関する国際的な共通認識の形成、相互運用性確保に向けた取組（標準化等）及びメタバース関連サービス提供者向けガイドライン（仮）の策定を、⑤～⑥の課題に対する取組の方向性としては市場、技術、ユーザー動向の継続的フォローアップ及びメタバースとUI/UXの関係等についての調査研究が挙げられる旨を整理した。また、2023年10月からは、同研究会の報告書において継続的なフォローアップが必要とされたものについての検討等を行う「安心・安全なメタバースの実現に関する研究会」を開催している。2023年4月の「G7群馬高崎デジタル・技術大臣会合」や同年5月の「G7広島サミット」において確認された民主的な価値に基づくメタバースの発展を念頭に、ユーザーにとってより安心・安全なメタバースを実現することを目的として、①メタバースの自主・自律的な発展に関する原則（オープン性・イノベーション、多様性・包摂性、リテラシー、コミュニティ）及び②メタバースの信頼性向上に関する原則（透明性・説明性、アカウンタビリティ、プライバシー、セキュリティ）からなる「メタバースの原則（1次案）」の検討等が行われてきたところである（本研究会については、本年夏頃に報告書を取りまとめ予定）。
　国際機関においてもメタバース等の没入型技術に関する検討が行われており、例えばOECDでは、2022年12月にGlobal Forum on Technology（GFTech）＊3の設置を公表し、没入型技術等についてフォーカスグループ（FG）を設置して議論をしている。没入型技術に関するFGでの議論は2023年12月から開始しており、2024年秋頃に報告書を取りまとめる予定となっている。また、総務省では、2023年10月に国連が主催した「インターネット・ガバナンス・フォーラム（IGF）京都」において、「民主的価値に基づくメタバースの実現」をテーマとしたセッションをOECDと共同開催するなど、国際的な議論に貢献する取組を進めている。

2 ロボティクス
　ロボティクスは従来我が国が強みを有する技術であり、特に産業用ロボットについては世界市場シェアの46％を占めている。また、労働人口減少が続く我が国においては、ロボティクス活用による生産性の向上、不足する労働力への対応、新たな産業創出等の期待も大きい。我が国では2015年度に「ロボット新戦略」を策定し、これまで30以上の官民連携による技術開発プロジェクトを実施してきており、ロボット自体やそれを支える個々の技術は進化してきている一方、ロボット導入現場のニーズとの間のギャップにより社会実装が進んでいないという実態もある。こうした状況を受け、国立研究開発法人新エネルギー・産業技術総合開発機構（NEDO）は2023年4月、ロボット技術戦略の策定およびプロジェクトの早期開始に向けて、社会課題の解決につながるロボット活用を推進するための方向性を大局的に整理・検討した「ロボット分野における研究開発と社会実装の大局的なアクションプラン」を公表した。アクションプランにおいては、ロボット活用が期待される8分野（ものづくり、食品製造、施設管理、小売・飲食、物流倉庫、農業、インフラ維持管理、建築）を取り上げ、あるべき姿の実現に向けて、2030年を目安に短期で求められる施策を「社会実装加速に向けたアクションプラン」、2035年に向けて中長期でのインパクト創出を見据えた施策を「次世代技術基盤構築に向けたアクションプラン」として取りまとめている。今後は、ロボットアクションプランとして抽出された取り組むべき技術開発と環境整備のアクションをもとに、将来の国家プロジェクト化や社会実装に向けた検討を進めていくこととしている。

3 自動運転技術
　自動運転技術の活用は、人口減少、高齢化等が進む地域の足を担う公共交通や物流の維持に寄与することが期待されており、社会利用拡大に向けた取組が求められている。政府は、「デジタル田園都市国家構想総合戦略（2023改訂版）」において、自動運転による地域交通を推進する観点から、関係府省庁が連携し、地域限定型の無人自動運転移動サービスを2025年度目途に50か所程度、2027年度までに100か所以上で実現する目標を掲げている。また、「デジタルライフライン全国総合整備計画」（経済産業省）においては、アーリーハーベストプロジェクトの1つに自動運転サービス支援道の設定が挙げられており、2024年度に新東名高速道路の一部区間等において100km以上の自動運転車優先レーンを設定し、自動運転トラックの運行の実現を目指すほか、2025年度までに全国50箇所、2027年度までに全国100箇所で自動運転車による移動サービス提供が実施できるようにすることを目指すとされている。この計画の実現に向け、警察庁、総務省、国土交通省等関係省庁が連携して取組が行われているところである。


2 サイバーセキュリティの確保に関する議論の動向
　デジタルテクノロジーを国民一人ひとりが安心して活用していくためには、サイバーセキュリティの確保も重要となる。近年、国際情勢の複雑化により、我が国を含む各国において政府機関等を狙ったサイバー攻撃が多く発生している状況にあることに加え、生成AI等のテクノロジーの登場により、利便性が増す一方で、それらの悪用によるリスクの拡大も指摘されている。従来、サイバーセキュリティは主にシステムの可用性や機密性を確保する、つまり、システムが停止しないようにすることや、データの窃取や漏洩を防ぐことに焦点が当てられ、ビジネスの連続性や利便性を確保してきた。これとともに、近年では情報の改ざん、偽・誤情報の拡散など、情報の中身の完全性、信頼性に関わる様々なリスクについても顕在化している。偽情報やディープフェイクの拡散、情報の改ざんや流出は、社会の信頼を揺るがし、社会の安定性や国家の安全保障にも影響を及ぼすだけでなく、政治的なプロセスや意思決定において深刻な影響を及ぼし、民主主義の健全性にとっても大きな脅威となる可能性がある。
　国家安全保障戦略（2022年12月）において「民間の重要インフラ等への国境を越えたサイバー攻撃、偽情報の拡散等を通じた情報戦等が恒常的に生起し、有事と平時の境目はますます曖昧になってきている」 と指摘するように、サイバー空間を巡る脅威はますます深刻化しており、いわば「常時有事」の状況となっているとも言える。こうした状況を踏まえ、さらなる情報通信ネットワークの安全性・信頼性の確保、サイバー攻撃への自律的な対処能力の向上、偽・誤情報への対応、国際連携の推進、普及啓発の推進に向けた取組が進められているところである。



