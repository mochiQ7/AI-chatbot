import os
from openai import OpenAI
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# prompt:GPTã«é€ã‚‹è³ªå•æ–‡
def generate_answer(context, question):
    prompt = f"""
ä»¥ä¸‹ã®æƒ…å ±ã‚’ã‚‚ã¨ã«ã€è³ªå•ã«å¯¾ã—ã¦ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„

ã€æƒ…å ±ã€‘
{context}

ã€è³ªå•ã€‘
{question}

ã€å›ç­”ã€‘
"""
    # promptã‚’GPTã«é€ä¿¡
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    # GPTã®è¿”äº‹ã‚’å–ã‚Šå‡ºã™
    return response.choices[0].message.content

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    from faiss_search import search

    while True:
        question = input("\n ğŸ˜¼è³ªå•ã‚’ã©ã†ãï¼(çµ‚äº†ã™ã‚‹ã«ã¯ã€exitã¨å…¥åŠ›)")
        if question.lower() == "exit":
            print("çµ‚äº†ã—ã¾ã™ã€‚ãŠç–²ã‚Œæ§˜ï¼ğŸ˜¼")
            break

        chunks = search(question)
        context = "\n".join(chunks) # æ¤œç´¢çµæœã‚’ã²ã¨ã¤ã®æ–‡å­—åˆ—ã«ã¾ã¨ã‚ã‚‹
    
        answer = generate_answer(context, question)
        print("å›ç­”:\n", answer)